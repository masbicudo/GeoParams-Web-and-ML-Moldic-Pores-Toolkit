{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying model learned from other image to detect moldic pores\n",
    "\n",
    "In this notebook we will apply the model learned from a different image to see what happens.\n",
    "The model was learned from an image in which all pores were classified in 3 classes: moldic, vugular and interparticle. In fact only the moldic class was used in the training process since it was the most abundant type of pores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.improc import colorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.models.EncoderNN import EncoderNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "model = EncoderNN().to(device)\n",
    "model.load_state_dict(torch.load(\"../models/supervised-1b.bin\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.improc import scale_image_and_save\n",
    "image_name = \"122.20_jpeg_escal\"\n",
    "path = f\"../data/thin_sections/{image_name}.jpg\"\n",
    "scale_image_and_save(path, \"../out/thin_sections_4x/\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"../out/thin_sections/{image_name}_25.jpg\"\n",
    "inputImage = cv2.imread(path)\n",
    "plt.imshow(inputImage[:,:,::-1])\n",
    "\n",
    "# BGR to CMKY:\n",
    "inputImageCMYK = colorspace.bgr2cmyk(inputImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryImage = cv2.inRange(\n",
    "    inputImageCMYK,\n",
    "    (92,   0,   0,   0),\n",
    "    (255, 255,  64, 196))\n",
    "binaryImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_ERODE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_ERODE, kernel, iterations=1)\n",
    "\n",
    "plt.imshow(binaryImage, cmap='gray')\n",
    "cv2.imwrite(\"../out/some.jpg\", binaryImage)\n",
    "porosidade = np.sum(binaryImage/255)/binaryImage.size\n",
    "print(f\"porosidade = {porosidade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "label_img = label(binaryImage)\n",
    "regions = regionprops(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objs = []\n",
    "for it, region in enumerate(regions):\n",
    "    ys = (region.coords.T[0] - label_img.shape[0]/2)/(label_img.shape[0]/2)\n",
    "    xs = (region.coords.T[1] - label_img.shape[1]/2)/(label_img.shape[1]/2)\n",
    "    obj = {\n",
    "        \"area\": region.area,\n",
    "        \"max-dist\": max((ys**2 + xs**2)**0.5),\n",
    "    }\n",
    "    all_objs.append(obj)\n",
    "\n",
    "df = pd.DataFrame(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(df[\"max-dist\"])\n",
    "pores_image3 = np.zeros(label_img.shape, dtype=np.uint8)\n",
    "for it, region in enumerate(regions):\n",
    "    if df[\"max-dist\"].iloc[it] <= max_dist*0.8:\n",
    "        color_value = 255\n",
    "        pores_image3[region.coords.T[0], region.coords.T[1]] = color_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pores_image3.shape)\n",
    "plt.imshow(pores_image3, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.models.WhitePixelRegionDataset import WhitePixelRegionDataset\n",
    "\n",
    "dataset2 = WhitePixelRegionDataset(\n",
    "    pores_image3, inputImage, None, num_samples=-1, seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_image = np.zeros(inputImage.shape, dtype=np.uint8)\n",
    "\n",
    "count_gt_half = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    from tqdm import tqdm\n",
    "    for it, (imgX, _, coords) in enumerate(tqdm(dataset2)):\n",
    "        # print(f\"coords.shape={coords.shape}\")\n",
    "        imgX = imgX.to(device)\n",
    "        imgX = imgX.unsqueeze(0)\n",
    "        imgX = imgX.permute(0, 3, 1, 2)\n",
    "        # print(f\"imgX.shape={imgX.shape}\")\n",
    "        imgX = imgX/255\n",
    "        imgX = F.interpolate(\n",
    "            imgX, size=(32, 32), mode='bilinear',\n",
    "            align_corners=False)\n",
    "        imgX = imgX.reshape(-1, 3*32*32)\n",
    "        # print(f\"imgX.shape={imgX.shape}\")\n",
    "        # break\n",
    "        Y = model(imgX)\n",
    "\n",
    "        pred_image[int(coords[0]), int(coords[1])] = float(Y[0,0])*255\n",
    "\n",
    "        # if float(Y[0,0]) > 0.5:\n",
    "        #     count_gt_half += 1\n",
    "        #     print(f\"{[coords[0], coords[1]]} -> {pred_image[coords[0], coords[1]]} (Y[0,0]={Y[0,0]})\")\n",
    "            \n",
    "        # if it > 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_image, vmin=0, vmax=255, cmap=\"gray\")\n",
    "cv2.imwrite(f\"../out/sup_pred_{image_name}_2.jpg\", pred_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
