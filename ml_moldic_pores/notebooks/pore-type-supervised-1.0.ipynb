{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pore type prediction from thin-section images 1.0\n",
    "\n",
    "In this notebook we explore the usage of thin-section patches to predict whether the central pixel of these patches are part of a moldic pore or not. This first attempt uses an Encoder Neural Network (EncoderNN from pre_sal_ii.models.nn) that has fully connected layers in which it reduces the amount of data in each layer until there is only 1 number in the output. I also tried a Reduction Convolutional Neural Network, but that didn't work, I suppose because it requires a lot of data to really converge.\n",
    "\n",
    "This notebook is in constant evolution, first I started with a simple model without validation, then I implemented cross-validation, which involved the creation of training classes and functions. These were moved to libraries for general usage in this project. To support K-folded cross validations on images, I also developed methods to partition the image in K parts that can be used as folded datasets instead of randomly distributing patches of the image between all folds. These were also moved to libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.improc import colorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting pores from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from pre_sal_ii.improc import scale_image_and_save, adjust_gamma\n",
    "\n",
    "import pre_sal_ii.models as models\n",
    "reload(models)\n",
    "models.set_all_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_original\"\n",
    "path = f\"../data/classificada_01/{image_name}.jpg\"\n",
    "scale_image_and_save(path, \"../out/classificada_01/\", 25)\n",
    "\n",
    "image_name = \"ML-tste_classidicada\"\n",
    "path = f\"../data/classificada_01/{image_name}.jpg\"\n",
    "scale_image_and_save(path, \"../out/classificada_01/\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_original\"\n",
    "path = f\"../out/classificada_01/{image_name}_25.jpg\"\n",
    "inputImage: np.ndarray = cv2.imread(path)\n",
    "inputImage = adjust_gamma(inputImage, 0.5)\n",
    "plt.imshow(inputImage[:,:,::-1])\n",
    "print(f\"inputImage shape: {inputImage.shape}\")\n",
    "\n",
    "# BGR to CMKY:\n",
    "inputImageCMYK = colorspace.bgr2cmyk(inputImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(C, M, Y, K) = (inputImageCMYK[..., 0],\n",
    "                inputImageCMYK[..., 1],\n",
    "                inputImageCMYK[..., 2],\n",
    "                inputImageCMYK[..., 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryImage = cv2.inRange(\n",
    "    inputImageCMYK,\n",
    "    (92,   0,   0,   0),\n",
    "    (255, 255,  64, 196))\n",
    "binaryImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_ERODE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "binaryImage = cv2.morphologyEx(binaryImage, cv2.MORPH_ERODE, kernel, iterations=1)\n",
    "\n",
    "plt.imshow(binaryImage, cmap='gray')\n",
    "cv2.imwrite(\"../out/some.jpg\", binaryImage)\n",
    "porosidade = np.sum(binaryImage/255)/binaryImage.size\n",
    "print(f\"porosidade = {porosidade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "label_img = label(binaryImage)\n",
    "regions = regionprops(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objs = []\n",
    "for it, region in enumerate(regions):\n",
    "    ys = (region.coords.T[0] - label_img.shape[0]/2)/(label_img.shape[0]/2)\n",
    "    xs = (region.coords.T[1] - label_img.shape[1]/2)/(label_img.shape[1]/2)\n",
    "    obj = {\n",
    "        \"area\": region.area,\n",
    "        \"max-dist\": max((ys**2 + xs**2)**0.5),\n",
    "    }\n",
    "    all_objs.append(obj)\n",
    "\n",
    "df = pd.DataFrame(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(df[\"max-dist\"])\n",
    "pores_image3 = np.zeros(label_img.shape, dtype=np.uint8)\n",
    "for it, region in enumerate(regions):\n",
    "    if df[\"max-dist\"].iloc[it] <= max_dist*0.8:\n",
    "        color_value = 255\n",
    "        pores_image3[region.coords.T[0], region.coords.T[1]] = color_value\n",
    "\n",
    "print(pores_image3.shape)\n",
    "plt.imshow(pores_image3, cmap='gray')\n",
    "cv2.imwrite(\"../out/binary_image3.jpg\", pores_image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(df[\"max-dist\"])\n",
    "print(label_img.shape)\n",
    "colored_image3 = np.zeros_like(inputImage, dtype=np.uint8)\n",
    "for it, region in enumerate(regions):\n",
    "    if df[\"max-dist\"].iloc[it] <= max_dist*0.8:\n",
    "        color_value = np.random.randint(0, 255, size=3)\n",
    "        colored_image3[region.coords.T[0], region.coords.T[1]] = color_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(colored_image3)\n",
    "cv2.imwrite(\"../out/colored_regions_rem_dist.jpg\", colored_image3[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading manually categorized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_classidicada\"\n",
    "path = f\"../out/classificada_01/{image_name}_25.jpg\"\n",
    "inputImage_cl = cv2.imread(path)\n",
    "plt.imshow(inputImage_cl[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryImage_clRed: np.ndarray = cv2.inRange(\n",
    "    inputImage_cl,\n",
    "    #  B,   G,   R\n",
    "    (  0,   0, 240),\n",
    "    (  5,   5, 255))\n",
    "plt.imshow(binaryImage_clRed, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"../out/binaryImage_clRed.jpg\", binaryImage_clRed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "label_img = label(binaryImage_clRed)\n",
    "regions = regionprops(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objs = []\n",
    "for it, region in enumerate(regions):\n",
    "    ys = (region.coords.T[0] - label_img.shape[0]/2)/(label_img.shape[0]/2)\n",
    "    xs = (region.coords.T[1] - label_img.shape[1]/2)/(label_img.shape[1]/2)\n",
    "    obj = {\n",
    "        \"area\": region.area,\n",
    "        \"max-dist\": max((ys**2 + xs**2)**0.5),\n",
    "    }\n",
    "    all_objs.append(obj)\n",
    "\n",
    "df = pd.DataFrame(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(df[\"max-dist\"])\n",
    "binaryImage_clRed_mx = np.zeros(label_img.shape, dtype=np.uint8)\n",
    "for it, region in enumerate(regions):\n",
    "    if df[\"max-dist\"].iloc[it] <= max_dist*0.8:\n",
    "        color_value = 255\n",
    "        binaryImage_clRed_mx[region.coords.T[0], region.coords.T[1]] = color_value\n",
    "\n",
    "print(binaryImage_clRed_mx.shape)\n",
    "plt.imshow(binaryImage_clRed_mx, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning images\n",
    "\n",
    "Here we use partitioning algorithms to partition training images in folds which are more or less equally distributed in measured area. Each algorithm has its own properties, and the one I liked the most was the K-Means Constrained Model. I also testes Split Regions Model, which just divides the image in rows containing approximately the same amount of pixels, and K-Means Model, which does not have the constraint of distributing areas more or less equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.training.image_clustering import cluster_pixels_kmeans_model\n",
    "from pre_sal_ii.training.image_clustering import cluster_pixels_kmeans_regions\n",
    "cp_model2 = cluster_pixels_kmeans_model(binaryImage_clRed_mx, n_regions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions2 = cluster_pixels_kmeans_regions(binaryImage_clRed_mx, cp_model2)\n",
    "regions_color11 = cv2.applyColorMap((regions2 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions2 = cluster_pixels_kmeans_regions(binaryImage_clRed, cp_model2)\n",
    "regions_color12 = cv2.applyColorMap((regions2 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions2 = cluster_pixels_kmeans_regions(pores_image3, cp_model2)\n",
    "regions_color13 = cv2.applyColorMap((regions2 * 30).astype(np.uint8), cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_color13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.training.image_clustering import cluster_pixels_h_splits_model\n",
    "from pre_sal_ii.training.image_clustering import cluster_pixels_h_splits_regions\n",
    "splits_mdl = cluster_pixels_h_splits_model(binaryImage_clRed_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions3 = cluster_pixels_h_splits_regions(binaryImage_clRed_mx, splits_mdl)\n",
    "regions_color21 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions3 = cluster_pixels_h_splits_regions(binaryImage_clRed, splits_mdl)\n",
    "regions_color22 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions3 = cluster_pixels_h_splits_regions(pores_image3, splits_mdl)\n",
    "regions_color23 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from pre_sal_ii.training.image_clustering import cluster_pixels_kmeans_constrained_model\n",
    "from pre_sal_ii.training.image_clustering import cluster_pixels_kmeans_constrained_regions\n",
    "\n",
    "cache_path = Path(\"../models/kmc_model_1.pkl\")\n",
    "cache_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "if cache_path.exists():\n",
    "    # Load cached model\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        kmc_model = pickle.load(f)\n",
    "    print(\"Loaded cached model from disk.\")\n",
    "else:\n",
    "    # Train the model\n",
    "    kmc_model = cluster_pixels_kmeans_constrained_model(binaryImage_clRed_mx, fraction=10)\n",
    "    # Save to cache\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(kmc_model, f)\n",
    "    print(\"Saved trained model to cache.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions3 = cluster_pixels_kmeans_constrained_regions(binaryImage_clRed_mx, kmc_model)\n",
    "regions_color31 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions3 = cluster_pixels_kmeans_constrained_regions(binaryImage_clRed, kmc_model)\n",
    "regions_color32 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "regions3 = cluster_pixels_kmeans_constrained_regions(pores_image3, kmc_model)\n",
    "regions_color33 = cv2.applyColorMap((regions3 * 30).astype(np.uint8), cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 9))\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(regions_color11[:,:,::-1])\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.imshow(regions_color21[:,:,::-1])\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.imshow(regions_color31[:,:,::-1])\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.imshow(regions_color12[:,:,::-1])\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.imshow(regions_color22[:,:,::-1])\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.imshow(regions_color32[:,:,::-1])\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.imshow(regions_color13[:,:,::-1])\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.imshow(regions_color23[:,:,::-1])\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.imshow(regions_color33[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pre_sal_ii.models.nn as nn_models\n",
    "import pre_sal_ii.models.ds as ds_models\n",
    "reload(ds_models)\n",
    "\n",
    "models.set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pre_sal_ii.improc import generate_region_map_from_centroids\n",
    "\n",
    "centroids = kmc_model.cluster_centers_\n",
    "regions4 = generate_region_map_from_centroids(np.ones_like(pores_image3), centroids)\n",
    "\n",
    "prob_base = pores_image3 / 255.0\n",
    "num_regions = 8\n",
    "\n",
    "from tqdm import tqdm\n",
    "prob_masks = []\n",
    "for i in tqdm(range(num_regions)):\n",
    "    mask_i = (regions4 == i).astype(float)\n",
    "    prob_masks.append(prob_base * mask_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(15, 6))\n",
    "for i in range(num_regions):\n",
    "    axes[i // 4, i % 4].imshow(prob_masks[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "fold_count = 8\n",
    "batch_size = 128\n",
    "num_samples = int(10000/(fold_count - 1)//batch_size*batch_size)\n",
    "\n",
    "print(f\"num_samples = {num_samples}\")\n",
    "print(f\"batch_size = {batch_size}\")\n",
    "print(f\"fold_count = {fold_count}\")\n",
    "\n",
    "if False:\n",
    "    datasets = [\n",
    "            ds_models.WhitePixelRegionDataset(\n",
    "                    prob_map, inputImage/255., binaryImage_clRed/255.,\n",
    "                    num_samples=num_samples, seed=42, use_img_to_tensor=True\n",
    "                ) for prob_map in prob_masks\n",
    "            ]\n",
    "else:\n",
    "    datasets = [\n",
    "        ds_models.ProbabilityMapPixelRegionDataset(\n",
    "                prob_map, inputImage/255., binaryImage_clRed/255.,\n",
    "                num_samples=num_samples,\n",
    "                region_size=101, target_region_size=1, seed=4290\n",
    "            ) for prob_map in prob_masks\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"get_whites_in_target = {sum([np.array(ds.get_whites_in_target()) for ds in datasets])}\")\n",
    "dataiter = iter(datasets[0])\n",
    "inputs = next(dataiter)\n",
    "(img, imgTarget, centerPixel) = inputs\n",
    "print(f\"len(data) = {len(inputs)}\")\n",
    "print(f\"min = {torch.min(img)}\", f\"max = {torch.max(img)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for it, inputs in enumerate(datasets[0]):\n",
    "    img = inputs[0]\n",
    "    imgTarget = inputs[1]\n",
    "    if it >= 10: break\n",
    "    # print(img.shape, imgTarget.shape)\n",
    "    img = img.permute(1,2,0)\n",
    "    assert img.shape == (101, 101, 3)\n",
    "    assert imgTarget.shape == (1, 1, 1)\n",
    "    axes[it//5*2+0, it%5].imshow(img.numpy()[:,:,::-1])\n",
    "    axes[it//5*2+1, it%5].imshow(imgTarget.numpy(), cmap=\"gray\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training\n",
    "\n",
    "In this section, we will setup the required objects for training:\n",
    "- device: cuda or cpu, preferring cuda if available\n",
    "- models: one model for each fold, all of the same class\n",
    "- criterion: loss function, which is used to get a vector with the direction of better values in parameter space\n",
    "- optimizers: one optimizer for each fold, they tell how to navigate the parameter space\n",
    "- trainer: one trainer for each fold, trainers are responsible for the training process of all epochs of a given fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "models = [nn_models.EncoderNN().to(device) for _ in range(fold_count)]\n",
    "criterion = nn.MSELoss()\n",
    "optimizers = [optim.AdamW(models[it].parameters(),\n",
    "                        lr=1e-4,\n",
    "                        weight_decay=1e-5,\n",
    "                       ) for it in range(fold_count)]\n",
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from pre_sal_ii.training import Trainer\n",
    "\n",
    "do_asserts = False\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def train_epoch_step(self, inputs):\n",
    "        imgs = inputs[0].to(self.device)\n",
    "        if do_asserts: assert (*imgs.shape[1:],) == (3, 101, 101)\n",
    "        imgs = F.interpolate(\n",
    "            imgs, size=(32, 32), mode='bilinear',\n",
    "            align_corners=False)\n",
    "        if do_asserts: assert (*imgs.shape[1:],) == (3, 32, 32)\n",
    "        imgs = imgs.reshape(-1, 3*32*32)\n",
    "        if do_asserts: assert (*imgs.shape[1:],) == (3*32*32,)\n",
    "        outputs = self.model(imgs)\n",
    "        return imgs.shape[0], outputs\n",
    "\n",
    "    def train_epoch_loss(self, inputs, outputs):\n",
    "        expected = inputs[1].to(self.device)\n",
    "        expected = torch.squeeze(expected, 1)\n",
    "        expected = torch.squeeze(expected, 2)\n",
    "        if do_asserts: assert (*expected.shape[1:],) == (1,)\n",
    "        loss = self.criterion(outputs, expected, **self.criterion_kwargs)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.training import cross_validate\n",
    "trainers = [MyTrainer(models[fold], optimizers[fold], criterion, device=device) for fold in range(fold_count)]\n",
    "print(\"Training...\")\n",
    "best_models, best_losses = cross_validate(trainers, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting and reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"models\": [m.state_dict() for m in best_models],\n",
    "    \"fold_losses\": best_losses,\n",
    "}, \"../models/supervised-8-folds-1.0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [nn_models.EncoderNN().to(device) for _ in range(fold_count)]\n",
    "checkpoint = torch.load(\"../models/supervised-8-folds-1.0.pt\")\n",
    "for i, m in enumerate(models2):\n",
    "    m.load_state_dict(checkpoint[\"models\"][i])\n",
    "fold_losses2 = checkpoint[\"fold_losses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best model to infer using whole image\n",
    "\n",
    "In this section, we will select all pixels which are within pores of the image. This means extracting a patch around each of these pixels and using the model to infer the output pixel classification as a chance of it being part of a moldic pore. We then render all of the pixel predictions in a single image and save it to later usage. We also combine the prediction with the ground-truth in a single image using green chanel for the ground-truth, and red chanel for the predictions. The correct predictions will appear in yellow (i.e. green + red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models2[np.argmin(fold_losses2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = ds_models.WhitePixelRegionDataset(\n",
    "    pores_image3, inputImage/255., binaryImage_clRed/255.,\n",
    "    num_samples=-1, seed=None, use_img_to_tensor=True)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_best = MyTrainer(model, None, None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_image = np.zeros_like(binaryImage_clRed, dtype=np.uint8)\n",
    "\n",
    "count_gt_half = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it, inputs in enumerate(tqdm(dataloader2)):\n",
    "        _, _, coords = inputs\n",
    "        step, outputs = trainer_best.train_epoch_step(inputs)\n",
    "        Y = outputs\n",
    "\n",
    "        xs = coords[:,1].cpu().numpy()\n",
    "        ys = coords[:,0].cpu().numpy()\n",
    "        vs = Y[:,0].cpu().numpy()\n",
    "        pred_image[ys, xs] = vs*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_image, vmin=0, vmax=255, cmap=\"gray\")\n",
    "cv2.imwrite(\"../out/sup_pred_8fold_1.0.jpg\", pred_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred_true = np.zeros([*binaryImage_clRed.shape, 3], dtype=np.uint8)\n",
    "image_pred_true = torch.tensor(image_pred_true, dtype=torch.uint8).permute(2, 0, 1)\n",
    "image_pred_true[1,:,:] = torch.tensor(binaryImage_clRed, dtype=torch.uint8)\n",
    "image_pred_true[2,:,:] = torch.tensor(pred_image, dtype=torch.uint8)\n",
    "image_pred_true = image_pred_true.permute(1, 2, 0)\n",
    "image_pred_true = image_pred_true.numpy()\n",
    "plt.imshow(image_pred_true[:,:,::-1])\n",
    "cv2.imwrite(\"../out/image_pred_8fold_true1.0.jpg\", image_pred_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pre-sal-ii-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
