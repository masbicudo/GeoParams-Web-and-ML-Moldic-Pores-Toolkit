{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eab17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.improc import colorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_sal_ii.improc import scale_image_and_save, adjust_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4aee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pre_sal_ii.improc.custom as custom\n",
    "import localizable_resources as lr\n",
    "\n",
    "def reload_libs_env():\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\".env\", override=True)\n",
    "\n",
    "    reload(custom)\n",
    "    reload(lr)\n",
    "\n",
    "reload_libs_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_original\"\n",
    "path = f\"../data/classificada_01/{image_name}.jpg\"\n",
    "scale_image_and_save(path, \"../out/classificada_01/\", 25)\n",
    "\n",
    "image_name = \"ML-tste_classidicada\"\n",
    "path = f\"../data/classificada_01/{image_name}.jpg\"\n",
    "scale_image_and_save(path, \"../out/classificada_01/\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aba394",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_original\"\n",
    "path = f\"../out/classificada_01/{image_name}_25.jpg\"\n",
    "inputImage = cv2.imread(path)\n",
    "gamma = 0.5\n",
    "inputImage = adjust_gamma(inputImage, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec46d8e",
   "metadata": {},
   "source": [
    "We need a binarized image to locate positions in the image that will be used as training data. I.e. only regions near white pixels will be used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5808dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pores = custom.proc_pores_basic(inputImage, gamma=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_pores, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88334109",
   "metadata": {},
   "source": [
    "We now identify areas in which we have moldic pores as labeled by Julia Favoreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"ML-tste_classidicada\"\n",
    "path = f\"../out/classificada_01/{image_name}_25.jpg\"\n",
    "binaryImage_clRed = custom.proc_moldic_pores(path)\n",
    "plt.imshow(binaryImage_clRed, cmap='gray')\n",
    "cv2.imwrite(\"../out/binaryImage_clRed.jpg\", binaryImage_clRed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_sal_ii.models.ds as ds\n",
    "reload(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pre_sal_ii.models.features import build_feature_stack\n",
    "image_prob_map = image_pores\n",
    "input_features = build_feature_stack(inputImage)\n",
    "num_samples = 10000\n",
    "dataset = ds.ProbabilityMapPixelRegionDataset(\n",
    "    image_prob_map, input_features, binaryImage_clRed/255., num_samples=num_samples,\n",
    "    region_size=101, target_region_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dataset[0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for it, (img, imgTarget, point) in enumerate(dataset):\n",
    "    if it >= 10: break\n",
    "    print(img.permute(1, 2, 0).shape)\n",
    "    img = img.permute(1, 2, 0)[:,:,0:3]\n",
    "    imgTarget = imgTarget.permute(1, 2, 0)\n",
    "    # print(img.numpy().shape, img.dtype)\n",
    "    axes[it//5*2+0, it%5].imshow(img.numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[it//5*2+1, it%5].imshow(imgTarget.numpy(), cmap=\"gray\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(dataset) = {len(dataset)}\")\n",
    "print(f\"dataset[0][0].shape = {dataset[0][0].shape}\")\n",
    "print(f\"dataset[0][1].shape = {dataset[0][1].shape}\")\n",
    "print(f\"max(dataset[0][0].flatten()) = {max(dataset[0][0].flatten())}\")\n",
    "print(f\"max(dataset[0][1].flatten()) = {max(dataset[0][1].flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b39696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import pre_sal_ii.models.nn as nn_models\n",
    "reload(nn_models)\n",
    "use_features = 3\n",
    "model = nn_models.EncoderNN(initial_dim=use_features*32*32).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd11d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dl:\n",
    "    print(k[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdaca18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "best_model_state = None\n",
    "best_model_loss = 0\n",
    "\n",
    "interpolate = lambda x: F.interpolate(\n",
    "    x, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "# TODO: try with lanczos ?\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "bar = tqdm(total=num_epochs*num_samples)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for features, gray_targets, _ in dl:\n",
    "        features = features[:,0:use_features,:,:]\n",
    "        # assert ((*features.shape[1:],) == (use_features, 101, 101))\n",
    "        features = interpolate(features.to(device)).view(-1, use_features*32*32)\n",
    "        # assert ((*features.shape[1:],) == (use_features*32*32,))\n",
    "        # assert (max(features.flatten()) <= 1.0)\n",
    "        \n",
    "        # assert ((*gray_targets.shape[1:],) == (1, 1, 1))\n",
    "        gray_targets = gray_targets.to(device)\n",
    "        # assert (max(gray_targets.flatten()) <= 1.0)\n",
    "        # assert ((*gray_targets.shape[1:],) == (1, 1, 1))\n",
    "\n",
    "        preds = model(features)\n",
    "        loss = criterion(preds, gray_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if best_model_state is None or loss.item() < best_model_loss:\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_model_loss = loss.item()\n",
    "\n",
    "        bar.update(32)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "dataset2 = ProbabilityMapPixelRegionDataset(\n",
    "    image_prob_map, input_features, None, num_samples=num_samples,\n",
    "    region_size=11)\n",
    "\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for it, (features, _, coords) in enumerate((dataset2)):\n",
    "        inputs = interpolate(features.to(device).unsqueeze(0))\n",
    "        features = inputs.view(-1, 33*32*32)\n",
    "        assert ((*features.shape,) == (1, 33*32*32))\n",
    "        Y = model(features)\n",
    "        assert ((*Y.shape,) == (1, 1))\n",
    "\n",
    "        sz = 50\n",
    "        x, y = coords[0], coords[1]\n",
    "        should_be = binaryImage_clRed[y-sz:y+sz+1, x-sz:x+sz+1]\n",
    "\n",
    "        if max(should_be.flatten()) < 255:\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(2, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(inputs[0, 0:3].squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(should_be)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(Y.cpu().numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20aeb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pre-sal-ii-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
